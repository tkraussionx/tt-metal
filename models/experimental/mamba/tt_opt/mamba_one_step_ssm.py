# SPDX-FileCopyrightText: Â© 2023 Tenstorrent Inc.

# SPDX-License-Identifier: Apache-2.0

import torch
import torch.nn.functional as F

import ttnn
from typing import Callable

from models.utility_functions import torch2tt_tensor
from models.helper_funcs import Linear
from models.experimental.mamba.reference.args import ModelArgs

def post_hook_to_print_output(operation, args, kwargs, output):
    if operation.name != "ttnn.unsqueeze_to_4D" or operation.name != "ttnn.reshape":
        print(f"Post-hook called for {operation.name}")
        print(f"Output shape: {output.shape}")
        # torch_output = ttnn.to_torch(output)
        # print(f"Output shape: {torch_output.shape}")
        # torch.save(torch_output, f"output_{operation.name}.pt")
def register_op_output(output, op_name):
    torch_output = ttnn.to_torch(output)
    print(f"Op name : {op_name}, Output shape: {torch_output.shape}")
    torch.save(torch_output, f"metal_{op_name}.pt")
class TtMambaSSM(torch.nn.Module):
    def __init__(self, args: ModelArgs, device, state_dict, num_users, hidden_size, configs):
        super().__init__()


        self.state_dict = state_dict
        self.device = device
        self.args = args

        # hidden state
        self.num_users = num_users
        self.hidden_size = hidden_size * 2
        self.configs = configs
        self.n = 32
        self.rank = self.args.dt_rank


        self.tt_hidden_state = ttnn.zeros(
                (1, 1, self.num_users, self.hidden_size*self.n),
                layout=ttnn.TILE_LAYOUT,
                device=self.device,
                memory_config=ttnn.DRAM_MEMORY_CONFIG,
            )

        """
        We need to split up the x_proj weights because in the reference
        implementation they perform the linear operation for dt, B, and C in a
        single step. Here we can't do that because it would involve fallback op
        slicing, so we break up the weights ahead of time and do the linear ops
        separately.
        """

        # delta rank weight
        print('***********using delta rank weight')
        x_proj_weight_name = "mixer.x_proj.weight"
        delta_t_proj = torch.transpose(self.state_dict[x_proj_weight_name][: self.args.dt_rank, :], -1, -2)
        self.delta_t_proj = ttnn.from_torch(
            delta_t_proj,
            layout=ttnn.TILE_LAYOUT,
            device=self.device,
            memory_config=ttnn.DRAM_MEMORY_CONFIG,
            dtype=ttnn.bfloat16,
        )

        # delta full weight
        print('***********using delta full weight')
        dt_proj_weight_name = "mixer.dt_proj.weight"
        dt_proj_bias_name = "mixer.dt_proj.bias"
        dt_proj_weights = torch.transpose(self.state_dict[dt_proj_weight_name], -1, -2)
        self.dt_proj_weights = ttnn.from_torch(
            dt_proj_weights,
            layout=ttnn.TILE_LAYOUT,
            device=self.device,
            memory_config=ttnn.DRAM_MEMORY_CONFIG,
            dtype=ttnn.bfloat16,
        )
        self.dt_proj_bias = ttnn.from_torch(
            self.state_dict[dt_proj_bias_name],
            layout=ttnn.TILE_LAYOUT,
            device=self.device,
            memory_config=ttnn.DRAM_MEMORY_CONFIG,
            dtype=ttnn.bfloat16,
        )

        # B
        print('***********using B weight')
        B_proj_weights = torch.transpose(self.state_dict[x_proj_weight_name][self.args.dt_rank : (self.args.dt_rank + self.args.d_state), :], -1, -2)
        # pad
        B_proj_weights = F.pad(B_proj_weights, (0, 16), "constant", 0)
        self.B_proj_weights = ttnn.from_torch(
            B_proj_weights,
            device=self.device,
            layout=ttnn.TILE_LAYOUT,
            memory_config=ttnn.DRAM_MEMORY_CONFIG,
            dtype=ttnn.bfloat16,
        )


        B_intermediate_tranform_weights = torch.eye(self.n).repeat(1, self.hidden_size).unsqueeze(0).unsqueeze(0)
        self.B_intermediate = ttnn.from_torch(
            B_intermediate_tranform_weights,
            layout=ttnn.TILE_LAYOUT,
            device=self.device,
            memory_config=ttnn.DRAM_MEMORY_CONFIG,
            dtype=ttnn.bfloat16,
        )

        # A
        print('***********using A weight')
        A_weight_name = "mixer.A_log"
        def preprocess_A(x):
            x = -torch.exp(x.float())
            # padding wit inf
            x = F.pad(x, (0, 16), "constant", float("-inf"))
            x = x.reshape(1, self.hidden_size*32)  # (1, 2en)
            return x.repeat(self.num_users, 1) # b, 2en

        A = preprocess_A(self.state_dict[A_weight_name])

        self.A = ttnn.from_torch(
            A,
            layout=ttnn.TILE_LAYOUT,
            device=self.device,
            memory_config=ttnn.DRAM_MEMORY_CONFIG,
            dtype=ttnn.bfloat16,
        )

        # C
        print('***********using C weight')
        x_proj_weight_name = "mixer.x_proj.weight"
        C_proj_weights = torch.transpose(self.state_dict[x_proj_weight_name][(self.args.dt_rank + self.args.d_state) :, :], -1, -2)
        # pad
        C_proj_weights = F.pad(C_proj_weights, (0, 16), "constant", 0)
        self.C_proj = ttnn.from_torch(
            C_proj_weights,
            layout=ttnn.TILE_LAYOUT,
            device=self.device,
            memory_config=ttnn.DRAM_MEMORY_CONFIG,
            dtype=ttnn.bfloat16,
        )


        # C pad
        C_pad = torch.zeros(1,1,self.num_users,self.n)
        self.C_pad = ttnn.from_torch(C_pad, layout=ttnn.TILE_LAYOUT, device=self.device, memory_config=ttnn.DRAM_MEMORY_CONFIG, dtype=ttnn.bfloat16)

        # D
        print('***********using D weight')
        D_weight_name = "mixer.D"
        D = self.state_dict[D_weight_name]
        D = D.repeat(self.num_users, 1)
        self.D = ttnn.from_torch(
            D,
            layout=ttnn.TILE_LAYOUT,
            device=self.device,
            dtype=ttnn.bfloat16,
            memory_config=ttnn.DRAM_MEMORY_CONFIG,
        )

    def forward(self, x):
        # delta
        delta_t_proj = ttnn.to_memory_config(self.delta_t_proj, memory_config=ttnn.L1_MEMORY_CONFIG)
        print('**********delta_t_proj shape', delta_t_proj.shape, x.shape)
        delta_t0 = ttnn.linear(x, delta_t_proj, memory_config=ttnn.L1_MEMORY_CONFIG)
        register_op_output(delta_t0, "delta_t_proj")
        ttnn.deallocate(delta_t_proj)

        dt_proj_weights = ttnn.to_memory_config(self.dt_proj_weights, memory_config=self.configs["sharded_rank"])
        delta_t1 = ttnn.linear(
            delta_t0, self.dt_proj_weights, bias=self.dt_proj_bias, memory_config=ttnn.L1_MEMORY_CONFIG
        )
        register_op_output(delta_t1, "dt_proj")
        ttnn.deallocate(delta_t0)
        ttnn.deallocate(dt_proj_weights)
        delta_t2 = ttnn.softplus(delta_t1, parameter1=1.0, parameter2=20.0, memory_config=ttnn.L1_MEMORY_CONFIG)
        register_op_output(delta_t2, "softplus")
        ttnn.deallocate(delta_t1)

        # calculate abar
        delta_t3 = ttnn.repeat_interleave(delta_t2, self.n, dim=3)
        delta_t4 = ttnn.to_memory_config(delta_t3, memory_config=self.configs["sharded_large"])
        ttnn.deallocate(delta_t3)
        abar0 = ttnn.to_memory_config(self.A, memory_config=self.configs["sharded_large"])
        abar1 = ttnn.mul(delta_t4, abar0, memory_config=self.configs["sharded_large"])
        ttnn.deallocate(abar0)
        ttnn.deallocate(delta_t4)
        register_op_output(abar1, "abar")
        abar2 = ttnn.to_memory_config(abar1, memory_config=ttnn.L1_MEMORY_CONFIG)
        ttnn.deallocate(abar1)
        abar3 = ttnn.exp(abar2)
        register_op_output(abar3, "abar_exp")
        ttnn.deallocate(abar2)
        abar4 = ttnn.to_memory_config(abar3, memory_config=self.configs["sharded_large"])
        ttnn.deallocate(abar3)

        # multiply abar and hidden_state
        hidden_state0 = ttnn.to_memory_config(self.tt_hidden_state, memory_config=self.configs["sharded_large"])
        amulh0 = ttnn.mul(abar4, hidden_state0, memory_config=self.configs["sharded_large"])
        register_op_output(amulh0, "abar_h")

        # deallocate abar and hidden_state
        ttnn.deallocate(abar4)
        ttnn.deallocate(hidden_state0)

        # B
        B_proj_weights = ttnn.to_memory_config(self.B_proj_weights, memory_config=ttnn.L1_MEMORY_CONFIG)
        B0 = ttnn.linear(x, B_proj_weights, memory_config=ttnn.L1_MEMORY_CONFIG)
        register_op_output(B0, "B_proj")
        ttnn.deallocate(B_proj_weights)
        B_intermediate_weights = ttnn.to_memory_config(self.B_intermediate, memory_config=ttnn.L1_MEMORY_CONFIG)
        B1 = ttnn.matmul(B0, B_intermediate_weights, memory_config=ttnn.L1_MEMORY_CONFIG) # 1, 1, 32, 32 -> 1, 1, 32, 32*2048
        ttnn.deallocate(B_intermediate_weights)
        ttnn.deallocate(B0)
        B2 = ttnn.to_memory_config(B1, memory_config=self.configs["sharded_large"])
        ttnn.deallocate(B1)

        # bbar
        delta_t3 = ttnn.repeat_interleave(delta_t2, self.n, dim=3) # 1, 1, 32, 2048 -> 1, 1, 32, 2048*32
        print('**********delta_t3 shape', delta_t3.shape, delta_t2.shape)
        ttnn.deallocate(delta_t2)
        delta_t4 = ttnn.to_memory_config(delta_t3, memory_config=self.configs["sharded_large"])
        ttnn.deallocate(delta_t3)
        print('**********delta_t4 shape', delta_t4.shape, B2.shape)
        bbar0 = ttnn.mul(delta_t4, B2, memory_config=self.configs["sharded_large"])
        register_op_output(bbar0, "bbar")
        ttnn.deallocate(delta_t4)
        ttnn.deallocate(B2)

        # multiply bbar and x
        x0 = ttnn.repeat_interleave(x, self.n, dim=3) # 1, 1, 32, 2048 -> 1, 1, 32, 2048*32
        x1 = ttnn.to_memory_config(x0, memory_config=self.configs["sharded_large"])
        ttnn.deallocate(x0)
        bmulx0 = ttnn.mul(bbar0, x1, memory_config=self.configs["sharded_large"])
        register_op_output(bmulx0, "bbar_x")

        # deallocate bbar
        ttnn.deallocate(bbar0)
        ttnn.deallocate(x1)

        # add amulh and bmulx
        hidden_state1 = ttnn.add(amulh0, bmulx0, memory_config=self.configs["sharded_large"])
        register_op_output(hidden_state1, "hidden_state")
        ttnn.deallocate(self.tt_hidden_state)
        self.tt_hidden_state = ttnn.to_memory_config(hidden_state1, memory_config=ttnn.DRAM_MEMORY_CONFIG)

        # deallocate amulh and bmulx
        ttnn.deallocate(amulh0)
        ttnn.deallocate(bmulx0)

        # compute C
        C_proj = ttnn.to_memory_config(self.C_proj, memory_config=ttnn.L1_MEMORY_CONFIG)

        C0 = ttnn.linear(x, C_proj, memory_config=ttnn.L1_MEMORY_CONFIG)  # 1, 1, b,n
        register_op_output(C0, "C_proj")
        ttnn.deallocate(C_proj)
        C1 = ttnn.permute(C0, (0, 2, 3, 1))  # b,n,1 #1,b,n,1
        register_op_output(C1, "C_permute")
        ttnn.deallocate(C0)
        '''
        print("**********C0 shape", C0.shape)
        C1 = ttnn.concat([C0, self.C_pad], dim=3)  # b,32
        C2 = ttnn.concat([self.C_pad, C0], dim=3)  # b,32
        ttnn.deallocate(C0)
        C3 = ttnn.permute(C1, (0, 2, 3, 1))  # b,32,1
        ttnn.deallocate(C1)
        C4 = ttnn.permute(C2, (0, 2, 3, 1))  # b,32,1
        ttnn.deallocate(C2)
        C5 = ttnn.concat([C3, C4], dim=3)  # b,32,2
        print("**********C5 shape", C5.shape)
        ttnn.deallocate(C3)
        ttnn.deallocate(C4)
        '''

        # hidden state @ C
        hidden_state2 = ttnn.to_memory_config(hidden_state1, memory_config=ttnn.L1_MEMORY_CONFIG)
        ttnn.deallocate(hidden_state1)
        hidden_state3 = ttnn.to_torch(hidden_state2)
        ttnn.deallocate(hidden_state2)
        hidden_state3 = hidden_state3.reshape(1, self.num_users, self.hidden_size, self.n)  # b, d, 32
        hidden_state3 = ttnn.from_torch(hidden_state3, layout=ttnn.TILE_LAYOUT, device=self.device, memory_config=ttnn.L1_MEMORY_CONFIG, dtype=ttnn.bfloat16)
        #hidden_state3 = ttnn.reshape(hidden_state2, (1, self.num_users, self.hidden_size, self.n))  # b, d, 32
        register_op_output(hidden_state3, "hidden_state_reshaped")
        C2 = ttnn.matmul(hidden_state3, C1)  # b, d, 1
        ttnn.deallocate(C1)
        C3 = ttnn.permute(C2, (0, 3, 1, 2)) # b, d
        register_op_output(C3, "C_h")
        ttnn.deallocate(C2)
        '''
        hidden_state2 = ttnn.to_memory_config(hidden_state1, memory_config=ttnn.L1_MEMORY_CONFIG)
        ttnn.deallocate(hidden_state1)
        hidden_state3 = ttnn.reshape(hidden_state2, (1, self.num_users, self.hidden_size // 2, 32))  # b, d/2, 32
        C6 = ttnn.matmul(hidden_state3, C5)  # b, d/2, 2
        print("**********C6 shape", C6.shape, hidden_state3.shape, C5.shape)
        ttnn.deallocate(hidden_state3)
        ttnn.deallocate(C5)
        C7 = ttnn.permute(C6, (0, 3, 2, 1))  # 2, d/2, b
        ttnn.deallocate(C6)
        C8 = ttnn.reshape(C7, (1, 1, self.hidden_size, self.num_users))  # 1, 1, d, b
        C9 = ttnn.permute(C8, (0, 1, 3, 2))  # 1, 1, b, d
        ttnn.deallocate(C7)
        ttnn.deallocate(C9)
        '''

        # x * D
        xD = ttnn.mul(x, self.D, memory_config=ttnn.L1_MEMORY_CONFIG)
        register_op_output(xD, "xD")
        ttnn.deallocate(x)

        # add xD and x
        print("**********xD shape", xD.shape, C3.shape)
        output = ttnn.add(xD, C3, memory_config=ttnn.L1_MEMORY_CONFIG)
        ttnn.deallocate(xD)
        ttnn.deallocate(C3)

        return output
