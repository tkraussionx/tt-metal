# SPDX-FileCopyrightText: Â© 2023 Tenstorrent Inc.

# SPDX-License-Identifier: Apache-2.0

import torch
import pytest
import tt_lib as ttl
from tests.tt_eager.python_api_testing.sweep_tests.comparison_funcs import comp_equal, comp_pcc
from models.utility_functions import skip_for_grayskull


@skip_for_grayskull("Requires eth connected devices to run")
@pytest.mark.parametrize(
    "input_shape, dim, layout",
    [
        # ([4, 1, 33, 256], 0, ttl.tensor.Layout.ROW_MAJOR),
        # ([4, 1, 256, 32], 0, ttl.tensor.Layout.TILE),
        # ([8, 5, 13, 384], 3, ttl.tensor.Layout.ROW_MAJOR),
        # ([8, 5, 32, 384], 3, ttl.tensor.Layout.TILE),
        # ([8, 8, 256, 384], 0, ttl.tensor.Layout.ROW_MAJOR),
        # ([8, 8, 256, 384], 0, ttl.tensor.Layout.TILE),
        # ([8, 8, 256, 384], 1, ttl.tensor.Layout.ROW_MAJOR),
        # ([8, 8, 256, 384], 1, ttl.tensor.Layout.TILE),
        # ([8, 8, 256, 384], 2, ttl.tensor.Layout.ROW_MAJOR),
        # ([8, 8, 256, 384], 2, ttl.tensor.Layout.TILE),
        # ([8, 8, 256, 384], 3, ttl.tensor.Layout.ROW_MAJOR),
        # ([8, 8, 256, 384], 3, ttl.tensor.Layout.TILE),
        # Only for BFP8B
        # ([1, 1, 640, 32768], 3, ttl.tensor.Layout.TILE),
        # MLP AllGather. Llama 2 decode attn, mlp. Llama2, Falcon 40B decode mlp attn
        # Mixtral 8x7B, functional bringup with expanded tensor getting allgathered
        # Full shape for 8 chips
        # ([1, 1, 32, 32768], 3, ttl.tensor.Layout.TILE),
        ([1, 1, 32, 32768], 3, ttl.tensor.Layout.ROW_MAJOR),
        # MLP AllGather. Llama 2 decode attn, mlp. Llama2, Falcon 40B decode mlp attn
        # Half shape for 4 chips, same per chip shape as 8 chips
        ([1, 1, 32, 16384], 3, ttl.tensor.Layout.TILE),
        ([1, 1, 32, 16384], 3, ttl.tensor.Layout.ROW_MAJOR),
        # Input, Selfout, Final AllGather. Llama2, Falcon 40B decode mlp attn
        # Full shape for 8 chips
        # ([1, 1, 32, 8192], 3, ttl.tensor.Layout.TILE),
        # ([1, 1, 32, 8192], 3, ttl.tensor.Layout.ROW_MAJOR),
        # Input, Selfout, Final AllGather. Llama2, Falcon 40B decode mlp attn
        # Half shape for running on 4 chips, same per chip shape as for 8 chips
        ([1, 1, 32, 4096], 3, ttl.tensor.Layout.TILE),
        ([1, 1, 32, 4096], 3, ttl.tensor.Layout.ROW_MAJOR),
        # Falcon 40B prefill
        # 8 chips
        ([1, 1, 2048, 8192], 3, ttl.tensor.Layout.TILE),
        ([1, 1, 2048, 8192], 3, ttl.tensor.Layout.ROW_MAJOR),
        # 4 chips, same per chip shape as 8 chips
        ([1, 1, 2048, 4096], 3, ttl.tensor.Layout.TILE),
        ([1, 1, 2048, 4096], 3, ttl.tensor.Layout.ROW_MAJOR),
        # Falcon 40B prefill
        # 8 chips
        ([1, 1, 2048, 32768], 3, ttl.tensor.Layout.TILE),
        ([1, 1, 2048, 32768], 3, ttl.tensor.Layout.ROW_MAJOR),
        # 4 chips, same per chip shape as 8 chips
        ([1, 1, 2048, 16384], 3, ttl.tensor.Layout.TILE),
        ([1, 1, 2048, 16384], 3, ttl.tensor.Layout.ROW_MAJOR),
        # Mixtral 8x7B, Min sequence length
        # 8 chips
        ([1, 1, 32768, 32768], 3, ttl.tensor.Layout.ROW_MAJOR),
        ([1, 1, 32768, 32768], 3, ttl.tensor.Layout.TILE),  # ultra slow?
        # 4 chips, per chip shape same as 8 chips
        ([1, 1, 32768, 16384], 3, ttl.tensor.Layout.ROW_MAJOR),
        ([1, 1, 32768, 16384], 3, ttl.tensor.Layout.TILE),
        # Llama galaxy mlp weights stationary -> emulation of row/col reduce
        ([1, 1, 128, 1024], 2, ttl.tensor.Layout.ROW_MAJOR),
        ([1, 1, 128, 1024], 2, ttl.tensor.Layout.TILE),
        # ([1, 1, 32, 8192], 3, ttl.tensor.Layout.ROW_MAJOR), # ALREADY LISTED PREVIOUSLY
        # ([1, 1, 32, 8192], 3, ttl.tensor.Layout.TILE),     # ALREADY LISTED PREVIOUSLY
        ([1, 1, 128, 4096], 2, ttl.tensor.Layout.ROW_MAJOR),  #
        ([1, 1, 128, 4096], 2, ttl.tensor.Layout.TILE),
        # ([1, 1, 32, 16384], 3, ttl.tensor.Layout.ROW_MAJOR), # duplicate of above. Update for 8 chip, actuall 32k for 8 chip but we are halving it for our 4 chip test
        # ([1, 1, 32, 16384], 3, ttl.tensor.Layout.TILE),      # duplicate of above. Update for 8 chip, actuall 32k for 8 chip but we are halving it for our 4 chip test
        ([1, 1, 8192, 32], 2, ttl.tensor.Layout.ROW_MAJOR),
        ([1, 1, 8192, 32], 2, ttl.tensor.Layout.TILE),
        ([1, 1, 1024, 128], 3, ttl.tensor.Layout.ROW_MAJOR),  # double on reduction dim for 8 chip
        ([1, 1, 1024, 128], 3, ttl.tensor.Layout.TILE),  # double on reduction dim for 8 chip
        ([1, 1, 16384, 32], 2, ttl.tensor.Layout.ROW_MAJOR),  # double on reduction dim for 8 chip
        ([1, 1, 16384, 32], 2, ttl.tensor.Layout.TILE),  # double on reduction dim for 8 chip
        ([1, 1, 4096, 128], 3, ttl.tensor.Layout.ROW_MAJOR),  # only for 4 chip
        ([1, 1, 4096, 128], 3, ttl.tensor.Layout.TILE),  # only for 4 chip
        ([1, 1, 128, 2048], 2, ttl.tensor.Layout.ROW_MAJOR),  # double on reduction dim for 8 chip
        ([1, 1, 128, 2048], 2, ttl.tensor.Layout.TILE),  # double on reduction dim for 8 chip
        # ([1, 1, 32, 8192], 3, ttl.tensor.Layout.ROW_MAJOR), # only for 4 chip
        # ([1, 1, 32, 8192], 3, ttl.tensor.Layout.TILE),      # only for 4 chip
        ([1, 1, 128, 8192], 2, ttl.tensor.Layout.ROW_MAJOR),  # double on reduction dim for 8 chip
        ([1, 1, 128, 8192], 2, ttl.tensor.Layout.TILE),  # double on reduction dim for 8 chip
    ],
)
@pytest.mark.parametrize(
    "input_dtype",
    [
        ttl.tensor.DataType.BFLOAT16,
        ttl.tensor.DataType.BFLOAT8_B,
    ],
)
@pytest.mark.parametrize(
    "mem_config",
    [
        ttl.tensor.MemoryConfig(buffer_type=ttl.tensor.BufferType.DRAM),
        ttl.tensor.MemoryConfig(buffer_type=ttl.tensor.BufferType.L1),
    ],
)
# Multi-Link currently hangs
@pytest.mark.parametrize(
    "num_links",
    [
        1,
    ],
)
def test_all_gather_interleaved(
    pcie_devices,
    input_shape,
    dim,
    num_links,
    input_dtype,
    layout,
    mem_config,
    use_program_cache,
):
    if num_links > 1:
        pytest.skip("Multi-Link not working")
    if layout == ttl.tensor.Layout.ROW_MAJOR and input_dtype == ttl.tensor.DataType.BFLOAT8_B:
        pytest.skip("Invalid combination")
    devices = pcie_devices
    input_tensor = torch.rand(input_shape).bfloat16()
    num_devices = len(devices)
    if num_devices < 2:
        pytest.skip("Requires multiple devices to run")
    elif num_devices == 2 and num_links == 2:
        pytest.skip("Not enough links to run")

    if input_shape[dim] % num_devices != 0 or (dim == 3 and input_shape[dim] // num_devices % 32 != 0):
        pytest.skip("Unsupported test case")

    input_tensors = torch.chunk(input_tensor, num_devices, dim)
    tt_input_tensors = []
    for i, t in enumerate(input_tensors):
        tt_input_tensors.append(ttl.tensor.Tensor(t, input_dtype).to(layout).to(devices[i], mem_config))

    tt_out_tensors = ttl.tensor.all_gather(tt_input_tensors, dim, num_links, output_mem_config=mem_config)

    for i, t in enumerate(tt_out_tensors):
        tt_output_tensor = t.cpu().to(ttl.tensor.Layout.ROW_MAJOR).to_torch()
        if input_dtype == ttl.tensor.DataType.BFLOAT16:
            eq, output = comp_equal(tt_output_tensor, input_tensor)
        else:
            eq, output = comp_pcc(tt_output_tensor, input_tensor)
        assert eq, f"{i} FAILED: {output}"
